{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Deep Learning with Apache Spark and Keras\n",
    "\n",
    "**justry** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "19 May 2017\r\n"
=======
<<<<<<< HEAD
      "19 May 2017\r\n"
=======
<<<<<<< HEAD
      "19 May 2017\r\n"
=======
<<<<<<< HEAD
      "19 May 2017\r\n"
=======
      "17 May 2017\r\n"
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     ]
    }
   ],
   "source": [
    "!(date +%d\\ %B\\ %G)\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This presentation will give the reader an introduction to the topic of distributed deep learning (DDL) and to the issues which need to be taken into consideration when applying this technique. We will also introduce a DDL framework based on a **fast and general engine for large-scale data processing** called [Apache Spark](https://spark.apache.org/) and the **neural network library** [Keras](https://keras.io). \n",
    "\n",
    "The project was initially initiated by the CMS experiment. CMS is exploring the possibility to use a deep learning model for the high level trigger in order to be able to handle the data rates for LHC run 3 and up. Furthermore, they would like to be able to train their models faster using distributed algorithms which allows them to tune their models with an increased frequency. An other requirement was those models should be trained on their complete dataset, which is in the order of a TB. At this point, production use-cases for ATLAS are also being evaluated. These focus more on the serving of models to classify instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction and problem statement](#Distributed-Deep-Learning,-an-introduction.)\n",
    "  - [Model parallelism](#Model-parallelism2)\n",
    "  - [Data parallelism](#Data-parallelism)\n",
    "- [Usage](#Distributed-Keras:-a-practicle-example)\n",
    "- [Acknowledgments](#Acknowledgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Deep Learning, an introduction.\n",
    "\n",
    "Unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. However, consider the problem of training a deep network with billions of parameters. How do we achieve this without waiting for days, or even weeks, and thus leaving more time to tune the model? Dean et al. [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) proposed an training paradigm which allows us to train a model on multiple physical machines. The authors describe two methods to achieve this, i.e., **data parallelism** and **model parallelism**<sup>1</sup>.\n",
    "\n",
    "### Model parallelism<sup><span style=\"font-size: 0.75em\">2</span></sup>\n",
    "\n",
    "In model parallelism a *single* model is distributed over multiple machines. The performance benefits of distributing a deep network across multiple machines depends mainly on the structure and of the model. Models with a large number of parameters typically benefit from access to more CPUs and memory, up to the point where communication costs, i.e., propagation of the weight updates and synchronization mechanisms, dominate [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf).\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/model_parallelism.png?raw=true\" alt=\"Model Parallelism\" width=\"300px\" />\n",
    "\n",
    "### Data parallelism\n",
    "\n",
    "As stated in the introduction, in order to train a large network in a reasonable amount of time, we need to parallize the optimization process (which is the learning of the model). In this settings, we take *several* model replicas, and distribute them over multiple machines. Of course, it would also be possible to combine this with the model parallelism approach. However, for the sake of simplicity, let us assume that a model (or several models) can be contained on a single machine. In order to parallelize the training, and to improve the usage of the resources of the cluster, we distribute the models over several machines.\n",
    "\n",
    "In order to build a distributed learning scheme using data parallelism, you would in the most simple case need at least 1 **parameter server**. A parameter server is basically a thread (or a collection of threads) which aggregate the incoming gradient updates of the workers into a so-called center variable, which acts as a *global consensus* variable. Finally, the weights which are stored in the center variable will eventually be used by the produced model.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/data_parallelism.png?raw=true\" alt=\"Data Parallelism\" width=\"400px\" />\n",
    "\n",
    "There are two general approaches towards solving data parallelism. The most straightforward is a **synchronous** method. In short, a synchronous data parallel method will wait for all workers to finish the current mini-batch or stochastic sample before continuing to the next iteration. Synchronous methods have the advantage that all workers will use the most recent center variable, i.e., a worker knows that all other workers will use the same center variable. However, the main disadvantage of this method is the synchronization itself. A synchronous method will never be truly synchronous. This is due to the many, and possibly different, machines. Furthermore, every machine could have a different workload, which would influence the training speed of a worker. As a result, synchronous methods need additional waiting mechanisms to synchronize all workers. These locking mechanisms will make sure that that all workers compute the next gradient based on the same center variable. However, locking mechanisms induce a significant wait which will significantly influence the training speed. For example, imagine a cluster node with an unusual high load. This high load will, due to CPU sharing, cause the training procedure to slow down. Which in turn, will cause the other workers to wait for this single node. Of course, this is just a simple and possibly extreme example, but this example shows how a single worker could significantly influence the training time of all workers.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/synchronous_method.png?raw=true\" alt=\"Data Parallelism\" width=\"500px\" />\n",
    "\n",
    "A very simple, but radical \"solution\" for this synchronization problem is to not synchronize the workers :) Workers simply fetch the center variable and update the parameter server with the computed gradient whenever a worker is ready. This approach is called an **asynchronous** data parallel method. Asynchronous methods, compared to synchronous methods, will have a different set of problems. One of these is a so-called **stale gradient**. This a gradient based on an older version of the center variable while the current center variable is a center variable which has already been updated by other workers. One approach to solve this is to induce and exponential decay factor to the gradient updates. However, this would waste computational resources, but of course, one could just get the most recent weights from the parameter server and then start again. However, as we will show later, it is actually stale gradients (result of asynchrony) that induce *implicit momentum* to the learning process [[2]](https://arxiv.org/pdf/1606.04487v4.pdf).\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/asynchronous_method.png?raw=true\" alt=\"Data Parallelism\" width=\"500px\" />\n",
    "\n",
    "At this point you probably ask the question: **why does this actually work?** A lot of people suggest this is due to the sparsity of the gradients. Intuitively, image having multiple workers processing different data (since every worker has its own dat partition), chances are the weights updates will be totally dissimilar since we are training a large network with a lot of tunable parameters. Furthermore, techniques such as dropout (if they are applied differently among the replicas) only increase the sparsity updates<sup>3</sup>.\n",
    "\n",
    "### Formalization\n",
    "\n",
    "We would also like to inform the reader that the general problem to be solved is the so-called **global consensus optimization** problem. A popular approach towards solving this is using the Alternating Direction Method of Multipliers (ADMM) [[3]](http://www.jmlr.org/proceedings/papers/v32/zhange14.pdf) [[4]](http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf). However, since this is outside the scope of this notebook we will not review this in-depth. But, we would like to note that the *Elastic Averaging* methods [[5]](https://arxiv.org/pdf/1412.6651.pdf) by Zhang et al., which we included in *Distributed Keras* are based on ADMM.\n",
    "\n",
    "<sub>**1:** Hybrids are possible as well.</sub>      \n",
    "<sub>**2:** This is mainly used for the computation of the network outputs [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf).</sub>           \n",
    "<sub>**3:** A way to check the sparsity between 2 gradients is to put all the weights in to a 1 dimensional vector, and then compute the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Keras\n",
    "\n",
    "Distributed Keras is a framework which uses Apache Spark and Keras. We chose for Spark because of the distributed environment. This allows us to preprocess the data in a distributed manner, and train our deep learning models on the same architecture, while still having the modeling simplicity of Keras.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Our architecture is very similar to the architecture discussed in [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf). However, we employ Apache Spark for data parallel reading and handling larger than memory datasets. The parameter server will always be created in the **Spark Driver**. This **is the program which creates the Spark Context**. For example, if the Jupyter installation of this notebook is running on the Spark cluster, then a cluster node will host the parameter server. However, if you run a Python script, which connects to a remote Spark cluster, then your computer will run the Spark Driver, and as a result will run the parameter server. In that case, be sure your network connection is able to handle the load, else your computer will be the bottleneck in the learning process.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/distkeras_architecture.png?raw=true\" alt=\"Model Parallelism\" width=\"500px\" />\n",
    "\n",
    "### Implementation of costum distributed optimizer\n",
    "\n",
    "In order to implement your own optimizer you need 2 classes. First, define your optimizer using the *Trainer* interface. We already supplied an *AsynchronousDistributedTrainer*, and an *SynchronousDistributedTrainer*. However, if you require an other procedure, please feel free to do so. Finally, you need a worker class. This class must have a *train* method with the required arguments, as specified by Apache Spark.\n",
    "\n",
    "### Usage\n",
    "\n",
    "In the following sections, we will give you an example how a complete workflow will look like. This includes setting up a Spark context, reading, preprocessing, and normalizing the data. Finally, we create a relatively simple model (feel free to adjust the parameters) with Keras and optimize it using the different distributed optimizers which are included by default.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We are using the ATLAS Higgs dataset constructed for the Kaggle machine learning challenge. This dataset is quite limited, it contains only **250000** instances. 40% of which we will be using as a test set. For future experiments, it would be usefull to integrate well understood datasets such as CIFAR or MNIST to evaluate against other optimizers. However, it would be nice to have a \"well understood\" HEP (High Energy Physics) dataset for this task :) "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
<<<<<<< HEAD
   "execution_count": 3,
=======
<<<<<<< HEAD
   "execution_count": 3,
=======
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *\n",
    "from distkeras.job_deployment import Job"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
<<<<<<< HEAD
   "execution_count": 4,
=======
<<<<<<< HEAD
   "execution_count": 4,
=======
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Notebook\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 6\n",
    "    num_cores = 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
<<<<<<< HEAD
   "execution_count": 5,
=======
<<<<<<< HEAD
   "execution_count": 5,
=======
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 6\n",
      "Number of desired cores / executor: 2\n",
      "Total number of workers: 12\n"
     ]
    }
   ],
   "source": [
    "# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired cores / executor: \" + `num_cores`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 5,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing a Spark Context\n",
    "\n",
    "In order to read our (big) dataset into our Spark Cluster, we first need a Spark Context. However, since Spark 2.0 there are some changes regarding the initialization of a Spark Context. For example, SQLContext and HiveContext do not have to be initialized separately anymore."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
<<<<<<< HEAD
   "execution_count": 7,
=======
<<<<<<< HEAD
   "execution_count": 7,
=======
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 6,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    #sc = SparkContext(conf=conf)\n",
    "    sc.conf = conf\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
<<<<<<< HEAD
   "execution_count": 8,
=======
<<<<<<< HEAD
   "execution_count": 8,
=======
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 7,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0ff7479efb15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Read the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"atlas_higgs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m(79)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     77 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 79 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     80 \u001b[0;31m            \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     81 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
<<<<<<< HEAD
   "execution_count": 10,
=======
<<<<<<< HEAD
   "execution_count": 10,
=======
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 8,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double-check the inferred schema, and get fetch a row to show how the dataset looks like.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preprocessing and normalization\n",
    "\n",
    "Since Spark's MLlib has some nice features for distributed preprocessing, we made sure we comply to the DataFrame API in order to ensure compatibility. What it basically boils down to, is that all the features (which can have different type) will be aggregated into a single column. More information on Spark MLlib (and other APIs) can be found here: [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "In the following steps we will show you how to extract the desired columns from the dataset and prepare the for further processing."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
<<<<<<< HEAD
   "execution_count": 11,
=======
<<<<<<< HEAD
   "execution_count": 11,
=======
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([138.47, 51.655, 97.827, 27.98, 0.91, 124.711, 2.666, 3.064, 41.928, 197.76, 1.582, 1.396, 0.2, 32.638, 1.017, 0.381, 51.626, 2.273, -2.414, 16.824, -0.277, 258.733, 2.0, 67.435, 2.15, 0.444, 46.062, 1.24, -2.475, 113.497]))]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 10,
=======
<<<<<<< HEAD
     "execution_count": 11,
=======
<<<<<<< HEAD
     "execution_count": 11,
=======
<<<<<<< HEAD
     "execution_count": 11,
=======
     "execution_count": 9,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)\n",
    "\n",
    "# Show what happened after applying the vector assembler.\n",
    "# Note: \"features\" column got appended to the end.\n",
    "dataset.select(\"features\").take(1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
<<<<<<< HEAD
   "execution_count": 12,
=======
<<<<<<< HEAD
   "execution_count": 12,
=======
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 10,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
<<<<<<< HEAD
     "execution_count": 11,
=======
<<<<<<< HEAD
     "execution_count": 12,
=======
<<<<<<< HEAD
     "execution_count": 12,
=======
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 10,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)\n",
    "#减小dataset的大小\n",
    "dataset = dataset[dataset['Weight']>7]\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
<<<<<<< HEAD
   "execution_count": 13,
=======
<<<<<<< HEAD
   "execution_count": 13,
=======
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 11,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
<<<<<<< HEAD
     "execution_count": 13,
=======
<<<<<<< HEAD
     "execution_count": 13,
=======
<<<<<<< HEAD
     "execution_count": 13,
=======
     "execution_count": 11,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[dataset['Weight']>7]\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import distkeras\n",
    "??distkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
=======
   "execution_count": 12,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_normalized: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print dir(dataset)</br>\n",
    "\n",
    "print dataset.select('Weight').take(5)</br>\n",
    "\n",
    "a = dataset.select('Weight').take(1)</br>\n",
    "\n",
    "print a</br>\n",
    "\n",
    "a = a[0]</br>\n",
    "\n",
    "print a</br>\n",
    "\n",
    "print type(a)</br>\n",
    "\n",
    "print dir(a)</br>\n",
    "\n",
    "print a.asDict()</br>\n",
    "\n",
    "print a['Weight']</br>\n",
    "\n",
    "print dataset</br>\n",
    "\n",
    "b = dataset[dataset['Weight']>6].count()</br>\n",
    "\n",
    "print b</br>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
<<<<<<< HEAD
   "execution_count": 15,
=======
<<<<<<< HEAD
   "execution_count": 15,
=======
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 13,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0)]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 14,
=======
<<<<<<< HEAD
     "execution_count": 15,
=======
<<<<<<< HEAD
     "execution_count": 15,
=======
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 13,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)\n",
    "\n",
    "# Show the result of the label transformation.\n",
    "dataset.select(\"Label\", \"label_index\").take(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
<<<<<<< HEAD
   "execution_count": 16,
=======
<<<<<<< HEAD
   "execution_count": 16,
=======
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 14,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some properties of the neural network for later use.\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
<<<<<<< HEAD
   "execution_count": 17,
=======
<<<<<<< HEAD
   "execution_count": 17,
=======
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 15,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label_index=0.0, newlabel=[1.0, 0.0])]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 16,
=======
<<<<<<< HEAD
     "execution_count": 17,
=======
<<<<<<< HEAD
     "execution_count": 17,
=======
<<<<<<< HEAD
     "execution_count": 17,
=======
     "execution_count": 15,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We observe that Keras is not able to work with these indexes.\n",
    "# What it actually expects is a vector with an identical size to the output layer.\n",
    "# Our framework provides functionality to do this with ease.\n",
    "# What it basically does, given an expected vector dimension, \n",
    "# it prepares zero vector with the specified dimensionality, and will set the neuron\n",
    "# with a specific label index to one. (One-Hot encoding)\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "\n",
    "transformer = OneHotTransformer(output_dim=nb_classes, input_col=\"label_index\", output_col=\"newlabel\")\n",
    "dataset = transformer.transform(dataset)\n",
    "# Only select the columns we need (less data shuffling) while training.\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"newlabel\")\n",
    "\n",
    "# Show the expected output vectors of the neural network.\n",
    "dataset.select(\"label_index\", \"newlabel\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: shuffling on a large dataset will take some time.\n",
    "\n",
    "We recommend users to first preprocess and shuffle their data, as is described in the data preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
<<<<<<< HEAD
   "execution_count": 18,
=======
<<<<<<< HEAD
   "execution_count": 18,
=======
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 16,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "# Note: we also support shuffling in the trainers by default.\n",
    "# However, since this would require a shuffle for every training we will only do it once here.\n",
    "# If you want, you can enable the training shuffling by specifying shuffle=True in the train() function."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
<<<<<<< HEAD
   "execution_count": 19,
=======
<<<<<<< HEAD
   "execution_count": 19,
=======
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 17,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features_normalized: vector, label_index: double, newlabel: array<double>]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 18,
=======
<<<<<<< HEAD
     "execution_count": 19,
=======
<<<<<<< HEAD
     "execution_count": 19,
=======
<<<<<<< HEAD
     "execution_count": 19,
=======
     "execution_count": 17,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(training_set, test_set) = dataset.randomSplit([0.6, 0.4])\n",
    "training_set.cache()\n",
    "test_set.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'path hdfs://master:9000/user/ubuntu/data_path/training_set.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
      "\u001b[0;32m<ipython-input-19-68cd3fbc38d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_path/training_set.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_path/test_set.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
=======
      "\u001b[0;32m<ipython-input-18-68cd3fbc38d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_path/training_set.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_path/test_set.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/Download/spark-2.1.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
      "\u001b[0;31mAnalysisException\u001b[0m: u'path hdfs://master:9000/user/ubuntu/data_path/training_set.parquet already exists.;'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/utils.py\u001b[0m(69)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
=======
<<<<<<< HEAD
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/utils.py\u001b[0m(69)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
=======
<<<<<<< HEAD
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/utils.py\u001b[0m(69)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
=======
<<<<<<< HEAD
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.1/python/pyspark/sql/utils.py\u001b[0m(69)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
=======
      "> \u001b[0;32m/home/ubuntu/Download/spark-2.1.0/python/pyspark/sql/utils.py\u001b[0m(69)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
      "\u001b[0;32m     67 \u001b[0;31m                                             e.java_exception.getStackTrace()))\n",
      "\u001b[0m\u001b[0;32m     68 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 69 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     70 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
<<<<<<< HEAD
      "ipdb> q\n"
=======
<<<<<<< HEAD
      "ipdb> q\n"
=======
<<<<<<< HEAD
      "ipdb> q\n"
=======
<<<<<<< HEAD
      "ipdb> q\n"
=======
      "\n",
      "KeyboardInterrupt\n"
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
     ]
    }
   ],
   "source": [
    "training_set.write.save(\"data_path/training_set.parquet\", format=\"parquet\")\n",
    "test_set.write.save(\"data_path/test_set.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model construction\n",
    "\n",
    "We will now construct a relatively simple Keras model (without any modifications) which, hopefully, will be able to classify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               15500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1002      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 267,002\n",
      "Trainable params: 267,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Worker Optimizer and Loss\n",
    "\n",
    "In order to evaluate the gradient on the model replicas, we have to specify an optimizer and a loss method. For this, we just follow the Keras API as defined in the documentation: [https://keras.io/optimizers/](https://keras.io/optimizers/) and [https://keras.io/objectives/](https://keras.io/objectives/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "In the following cells we will train and evaluate the model using different distributed trainers, however, we will as well provide a baseline metric using a **SingleTrainer**, which is basically an instance of the Adagrad optimizer running on Spark.\n",
    "\n",
    "Furthermore, we will also evaluate every training using Spark's MulticlassClassificationEvaluator [https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "\n",
    "We will evaluate all algorithms using the F1 [https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score) and accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model):\n",
    "    global test_set\n",
    "    \n",
    "    # Allocate a Distributed Keras Accuracy evaluator.\n",
    "    evaluator = AccuracyEvaluator(prediction_col=\"prediction_index\", label_col=\"label_index\")\n",
    "    # Clear the prediction column from the testset.\n",
    "    test_set = test_set.select(\"features_normalized\", \"label_index\", \"newlabel\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    test_set = predictor.predict(test_set)\n",
    "    # Allocate an index transformer.\n",
    "    index_transformer = LabelIndexTransformer(output_dim=nb_classes)\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    test_set = index_transformer.transform(test_set)\n",
    "    # Fetch the score.\n",
    "    score = evaluator.evaluate(test_set)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_result(trainer, accuracy, dt):\n",
    "    global results;\n",
    "    \n",
    "    # Store the metrics.\n",
    "    results[trainer] = {}\n",
    "    results[trainer]['accuracy'] = accuracy;\n",
    "    results[trainer]['time_spent'] = dt\n",
    "    # Display the metrics.\n",
    "    print(\"Trainer: \" + str(trainer))\n",
    "    print(\" - Accuracy: \" + str(accuracy))\n",
    "    print(\" - Training time: \" + str(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we will allocate a simple datastructure which will hold the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SingleTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***SingleTrainer*** is used as a benchmarking trainer to compare to distributed trainer. However, one could also use this trainer if the dataset is too big to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = SingleTrainer(keras_model=model, worker_optimizer=optimizer,\n",
    "                        loss=loss, features_col=\"features_normalized\",\n",
    "                        label_col=\"newlabel\", num_epoch=1, batch_size=32)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: single\n",
      " - Accuracy: 1.0\n",
      " - Training time: 2.69969701767\n"
     ]
    }
   ],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('single', accuracy, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous EASGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EASGD based methods, proposed by Zhang et al., transmit the complete parametrization instead of the gradient. These methods will then \"average\" the difference of the center variable and the backpropagated worker variable. This is used to compute a new master variable, on which the worker nodes will base their backpropagation in the next iteration on.\n",
    "\n",
    "Asynchronous EASGD will do this in an asynchronous fashion, meaning, whenever a worker node is done processing its mini-batch after a certain amount of iterations (communication window), then the computed parameter will be communicated with the parameter server, which will update the center (master) variable immediately without waiting for other workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = 1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
<<<<<<< HEAD
   "execution_count": 26,
=======
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 25,
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
      "  File \"/home/ubuntu/Download/dist-keras/distkeras/job_deployment.py\", line 356, in run\n",
      "    self.destroy_remote_job()\n",
      "  File \"/home/ubuntu/Download/dist-keras/distkeras/job_deployment.py\", line 323, in destroy_remote_job\n",
      "    self.trained_model = deserialize_keras_model(model)\n",
      "  File \"/home/ubuntu/Download/dist-keras/distkeras/utils.py\", line 126, in deserialize_keras_model\n",
      "    model.set_weights(weights)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 697, in set_weights\n",
      "    self.model.set_weights(weights)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 1935, in set_weights\n",
      "    K.batch_set_value(tuples)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2027, in batch_set_value\n",
      "    get_session().run(assign_ops, feed_dict=feed_dict)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 933, in _run\n",
      "    + e.args[0])\n",
      "TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_2:0\", shape=(500, 500), dtype=float32) is not an element of this graph.\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
=======
      "  File \"/home/ubuntu/Download/dist-keras/distkeras/job_deployment.py\", line 354, in run\n",
      "    while not self.is_finished():\n",
      "  File \"/home/ubuntu/Download/dist-keras/distkeras/job_deployment.py\", line 312, in is_finished\n",
      "    response = urllib2.urlopen(request)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 154, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 429, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 447, in _open\n",
      "    '_open', req)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 407, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 1228, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/urllib2.py\", line 1198, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error [Errno 111] Connection refused>\n",
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = AEASGD(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers, \n",
    "                 batch_size=32, features_col=\"features_normalized\", label_col=\"newlabel\", num_epoch=1,\n",
    "                 communication_window=32, rho=5.0, learning_rate=0.1)\n",
    "trainer.set_parallelism_factor(1)\n",
    "job = Job(\"3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA\",\n",
    "          \"user1\",\n",
    "          \"data_path/training_set.parquet\",\n",
    "          1,\n",
    "          16,\n",
    "          trainer)\n",
<<<<<<< HEAD
    "job.send('http://ec2-52-79-121-94.ap-northeast-2.compute.amazonaws.com:8000')\n",
=======
<<<<<<< HEAD
    "job.send('http://ec2-52-79-121-94.ap-northeast-2.compute.amazonaws.com:8000')\n",
=======
<<<<<<< HEAD
    "job.send('http://ec2-52-79-121-94.ap-northeast-2.compute.amazonaws.com:8000')\n",
=======
<<<<<<< HEAD
    "job.send('http://ec2-52-79-121-94.ap-northeast-2.compute.amazonaws.com:8000')\n",
=======
    "job.send('http://ec2-13-124-129-21.ap-northeast-2.compute.amazonaws.com:8000')\n",
>>>>>>> 36262834119de2c4b4420e00ded0280df0cbc69c
>>>>>>> 2b09b792d55ee69339859af4af996e2b57d4cde1
>>>>>>> e3f94d75c2ca136c1b6e55e522da926daaad2bc6
>>>>>>> 99246d56a3d196e3ea31040262bae82387d965f6
    "job.wait_completion()\n",
    "trained_model = job.get_trained_model()\n",
    "history = job.get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: aeasgd\n",
      " - Accuracy: 1.0\n",
      " - Training time: 5.7217400074\n"
     ]
    }
   ],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('aeasgd', accuracy, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous EAMSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between asynchronous EAMSGD and asynchronous EASGD is the possibility of specifying an explicit momentum term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = EAMSGD(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers,\n",
    "                 batch_size=32, features_col=\"features_normalized\", label_col=\"newlabel\", num_epoch=1,\n",
    "                 communication_window=32, rho=5.0, learning_rate=0.1, momentum=0.6)\n",
    "trainer.set_parallelism_factor(1)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('eamsgd', accuracy, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DOWNPOUR SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = DOWNPOUR(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers,\n",
    "                   batch_size=32, communication_window=5, num_epoch=1,\n",
    "                   features_col=\"features_normalized\", label_col=\"newlabel\")\n",
    "trainer.set_parallelism_factor(1)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('downpour', accuracy, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental observations\n",
    "\n",
    "- DOWNPOUR converges well when a small communication window is used $< 5$.\n",
    "- EASGD based methods on the other hand, thrive using large communication windows $> 25$.\n",
    "- Asynchronous methods induce implicit momentum.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Distributed Deep Learning can **significantly speedup** the learning process. We provide such a framework built on top of Keras and Apache Spark. The latter provides a nice framework for distributed data processing and model evaluation. We can easily integrate our workflows using Apache Spark, and thus speeding up the **data preprocessing**, and our **model optimization procedure** while still having the same **modelling simplicity**.\n",
    "\n",
    "Our group is always open on further collaboration on this work, and would like to assist the physics community in their machine learning efforts.\n",
    "\n",
    "**Contact**: [joeri.hermans@cern.ch](mailto:joeri.hermans@cern.ch)\n",
    "             [luca.canali@cern.ch](mailto:luca.canali@cern.ch)\n",
    "             [zbigniew.baranowski@cern.ch](mailto:zbigniew.baranowski@cern.ch)\n",
    "\n",
    "## Future work\n",
    "\n",
    "- Understanding \"theoretical\" meaning of a communication window.\n",
    "- Apply compression for big weight updates when sending updates to the parameter server.\n",
    "- Keep track of a gradient residual (this will reduce the bandwidth due to sparsity).\n",
    "- Evaluation of algorithms with GPU's.\n",
    "- Optimization of parameter sharing (e.g., sockets instead of REST API).\n",
    "- Use \"famous\" ConvNet architectures with well known datasets in order to have a more sound evaluation.\n",
    "- Add threaded queue to process asynchronous updates.\n",
    "- Training accuracy while training.\n",
    "- Stop on target loss.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Many thanks to Zbigniew Baranowski and Luca Canali of the IT-DB group, and to Jean-Roch Vlimant, Maurizio Pierini, and Federico Presutti of the EP-UCM group for their collaboration on this work.\n",
    "\n",
    "## GitHub repository\n",
    "\n",
    "[https://github.com/JoeriHermans/dist-keras/](#https://github.com/JoeriHermans/dist-keras)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
