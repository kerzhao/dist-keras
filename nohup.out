[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[I 09:27:41.725 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret
[W 09:27:41.754 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 09:27:41.762 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 09:27:41.762 NotebookApp] 0 active kernels 
[I 09:27:41.762 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 09:27:41.762 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 09:27:48.320 NotebookApp] 302 GET /api/sessions?_=1494562070292 (218.241.251.148) 1.26ms
[I 09:27:48.326 NotebookApp] 302 GET /api/terminals?_=1494562070293 (218.241.251.148) 0.65ms
[I 09:27:49.255 NotebookApp] 302 GET /tree/bin (218.241.251.148) 0.46ms
[I 09:27:52.927 NotebookApp] 302 POST /login?next=%2Ftree%2Fbin (218.241.251.148) 0.97ms
[W 09:27:53.108 NotebookApp] 404 GET /tree/bin (218.241.251.148) 6.53ms referer=http://ec2-52-79-153-231.ap-northeast-2.compute.amazonaws.com:1427/login?next=%2Ftree%2Fbin
[I 09:27:59.398 NotebookApp] 302 GET / (218.241.251.148) 0.37ms
[W 09:28:04.029 NotebookApp] 404 GET /edit/bin/wingdb.py (218.241.251.148): File does not exist: bin/wingdb.py
[W 09:28:04.029 NotebookApp] 404 GET /edit/bin/wingdb.py (218.241.251.148) 1.29ms referer=http://ec2-52-79-153-231.ap-northeast-2.compute.amazonaws.com:1427/tree/bin
[C 09:56:37.433 NotebookApp] received signal 15, stopping
[I 09:56:37.433 NotebookApp] Shutting down kernels
[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[I 01:29:35.764 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret
[W 01:29:35.785 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 01:29:35.791 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 01:29:35.791 NotebookApp] 0 active kernels 
[I 01:29:35.791 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 01:29:35.791 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 01:29:37.700 NotebookApp] 302 GET / (218.241.251.148) 0.50ms
[I 01:29:37.780 NotebookApp] 302 GET /tree? (218.241.251.148) 0.65ms
[I 01:29:42.385 NotebookApp] 302 POST /login?next=%2Ftree%3F (218.241.251.148) 0.93ms
[I 01:30:25.384 NotebookApp] Creating new notebook in /scripts
[I 01:30:31.139 NotebookApp] Kernel started: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[W 01:30:41.312 NotebookApp] Timeout waiting for kernel_info reply from 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9
[I 01:32:31.303 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:34:31.311 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:36:31.218 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:48:31.271 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:50:31.189 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:56:31.203 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 01:57:23.020 NotebookApp] Kernel interrupted: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9
[I 01:58:31.196 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 02:02:31.227 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 02:04:31.207 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 02:06:31.198 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 02:19:02.762 NotebookApp] Kernel interrupted: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9
[I 02:20:31.307 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 02:26:31.772 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 02:44:02.321 NotebookApp] Replacing stale connection: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9:5EC2FAAAD91F46839A3C9596C499F325
[W 02:45:58.301 NotebookApp] Replacing stale connection: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9:5EC2FAAAD91F46839A3C9596C499F325
[I 02:48:33.187 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[C 03:00:56.828 NotebookApp] received signal 15, stopping
[I 03:00:56.829 NotebookApp] Shutting down kernels
[I 03:00:56.930 NotebookApp] Kernel shutdown: 401bcdfc-69bb-4553-8a4c-58fd59a5b1b9
[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[I 03:02:44.559 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret
[W 03:02:44.588 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 03:02:44.597 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 03:02:44.597 NotebookApp] 0 active kernels 
[I 03:02:44.598 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 03:02:44.598 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 03:02:44.669 NotebookApp] Couldn't authenticate WebSocket connection
[W 03:02:44.704 NotebookApp] 403 GET /api/kernels/401bcdfc-69bb-4553-8a4c-58fd59a5b1b9/channels?session_id=5EC2FAAAD91F46839A3C9596C499F325 (218.241.251.148) 35.94ms referer=None
[I 03:02:51.409 NotebookApp] 302 GET /api/sessions?_=1494811827729 (218.241.251.148) 1.07ms
[I 03:02:51.413 NotebookApp] 302 GET /api/terminals?_=1494811827730 (218.241.251.148) 0.65ms
[I 03:02:52.608 NotebookApp] 302 GET /tree/scripts (218.241.251.148) 0.41ms
[I 03:02:55.582 NotebookApp] 302 POST /login?next=%2Ftree%2Fscripts (218.241.251.148) 0.77ms
[I 03:02:59.435 NotebookApp] Kernel started: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 03:14:59.810 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 03:34:06.535 NotebookApp] Replacing stale connection: db4385d8-5639-41a0-b7c3-53ff6332894a:E0052BE53C774CB9834944047F7A9EEB
[W 03:35:41.536 NotebookApp] WebSocket ping timeout after 90000 ms.
[W 03:46:50.315 NotebookApp] WebSocket ping timeout after 118315 ms.
[W 03:46:54.545 NotebookApp] Replacing stale connection: db4385d8-5639-41a0-b7c3-53ff6332894a:E0052BE53C774CB9834944047F7A9EEB
[I 03:54:04.969 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 03:54:27.145 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 03:54:31.426 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 03:54:39.071 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 03:55:28.776 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 03:55:29.434 NotebookApp] KernelRestarter: restarting kernel (1/5)
WARNING:root:kernel db4385d8-5639-41a0-b7c3-53ff6332894a restarted
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 03:55:34.324 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[W 03:55:37.192 NotebookApp] Replacing stale connection: db4385d8-5639-41a0-b7c3-53ff6332894a:7087F1F57A64449B9731BF2F53CCE2D5
[I 03:56:31.307 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 03:59:58.210 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 04:00:31.062 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:00:39.569 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 04:00:52.042 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 04:01:04.138 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:01:45.872 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:02:31.402 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 04:03:35.818 NotebookApp] Notebook examples/workflow.ipynb is not trusted
[I 04:03:37.059 NotebookApp] Kernel started: 8518fcd9-e370-4f59-a2db-3f962959aa5a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 04:03:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 04:03:48.143 NotebookApp] Kernel started: 1a4465bd-418a-49de-bc47-c580d58b6e89
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 04:03:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/05/15 04:03:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[I 04:03:59.689 NotebookApp] Kernel started: 4c78ff52-23b5-4f05-b9e4-6a342c26483d
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 04:04:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/05/15 04:04:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/05/15 04:04:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[I 04:06:44.761 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:08:31.069 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:10:31.052 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:11:51.642 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 04:12:31.064 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:12:36.359 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:13:05.786 NotebookApp] Kernel interrupted: db4385d8-5639-41a0-b7c3-53ff6332894a
[I 04:13:11.362 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:13:36.537 NotebookApp] Kernel restarted: db4385d8-5639-41a0-b7c3-53ff6332894a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:13:40.210 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:13:40.590 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:13:48.309 NotebookApp] Kernel shutdown: 1a4465bd-418a-49de-bc47-c580d58b6e89
[I 04:13:48.712 NotebookApp] Kernel shutdown: 8518fcd9-e370-4f59-a2db-3f962959aa5a
[I 04:13:49.415 NotebookApp] Kernel shutdown: 4c78ff52-23b5-4f05-b9e4-6a342c26483d
[C 04:14:31.862 NotebookApp] received signal 15, stopping
[I 04:14:31.863 NotebookApp] Shutting down kernels
[I 04:14:32.873 NotebookApp] Kernel shutdown: db4385d8-5639-41a0-b7c3-53ff6332894a
[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[W 04:14:48.324 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 04:14:48.329 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 04:14:48.329 NotebookApp] 0 active kernels 
[I 04:14:48.329 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 04:14:48.329 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 04:14:54.256 NotebookApp] Kernel started: 37d45684-5943-4ffc-88f4-45db0999468a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:15:19.696 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:16:21.750 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:16:54.357 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:30:54.420 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:38:54.755 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:39:40.942 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:40:55.211 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:43:38.390 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:44:54.414 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:52:43.263 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:52:54.309 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:53:05.416 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:54:26.774 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:54:57.718 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:55:02.643 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[W 04:55:02.824 NotebookApp] 404 GET /gen204?invalidResponse=q=https%3A%2F%2Ftranslate.googleapis.com%2Ftranslate_a%2Fsingle%3Fclient%3Dgtx%26s,ql=157,r=,rl=0 (218.241.251.148) 6.09ms referer=http://ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427/notebooks/scripts/create%20and%20load%20secret%20file.ipynb
[I 04:55:44.585 NotebookApp] Kernel restarted: 37d45684-5943-4ffc-88f4-45db0999468a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:55:53.336 NotebookApp] Kernel interrupted: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:55:56.585 NotebookApp] KernelRestarter: restarting kernel (1/5)
WARNING:root:kernel 37d45684-5943-4ffc-88f4-45db0999468a restarted
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:55:59.269 NotebookApp] Kernel restarted: 37d45684-5943-4ffc-88f4-45db0999468a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[W 04:56:02.953 NotebookApp] Replacing stale connection: 37d45684-5943-4ffc-88f4-45db0999468a:490C6603CBBE433B86888918D78A2626
[I 04:56:54.447 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 04:57:03.611 NotebookApp] Kernel restarted: 37d45684-5943-4ffc-88f4-45db0999468a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:57:34.465 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 04:58:06.768 NotebookApp] Replacing stale connection: 37d45684-5943-4ffc-88f4-45db0999468a:490C6603CBBE433B86888918D78A2626
[E 04:58:08.398 NotebookApp] Exception in callback <bound method ZMQChannelsHandler.send_ping of ZMQChannelsHandler(37d45684-5943-4ffc-88f4-45db0999468a)>
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py", line 1041, in _run
        return self.callback()
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py", line 197, in send_ping
        self.ping(b'')
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/websocket.py", line 260, in ping
        raise WebSocketClosedError()
    WebSocketClosedError
[I 04:58:43.218 NotebookApp] Kernel restarted: 37d45684-5943-4ffc-88f4-45db0999468a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 04:59:34.744 NotebookApp] Kernel shutdown: 37d45684-5943-4ffc-88f4-45db0999468a
[I 04:59:37.450 NotebookApp] Kernel started: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 05:01:37.538 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:02:32.692 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:03:37.539 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:03:57.369 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:05:38.265 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:22:34.073 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:22:40.205 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:22:45.114 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 05:23:11.513 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:23:37.596 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:24:00.318 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 05:25:37.550 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:27:40.292 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:47:41.453 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:49:37.885 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 05:49:38.158 NotebookApp] Replacing stale connection: d77ced55-60db-4f58-be70-02fec45bfac1:80B51CEAAF294329951590B6EB139E1E
[E 05:49:39.667 NotebookApp] Uncaught exception GET /api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E (218.241.251.148)
    HTTPServerRequest(protocol='http', host='ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', method='GET', uri='/api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E', version='HTTP/1.1', remote_ip='218.241.251.148', headers={'Origin': 'http://ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Upgrade': 'websocket', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch', 'Sec-Websocket-Version': '13', 'Host': 'ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Sec-Websocket-Key': 'RX8bqMNwKHVVYSj0jXM1aA==', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36', 'Connection': 'Upgrade', 'Cookie': '_xsrf=2|0144bb65|d2553a8ba249d96d8df5e6491fe6d9dd|1494811777; username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427="2|1:0|10:1494817375|67:username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427|44:OWVkYzZjNmY3MGU4NDI0YmFiMDQ2OGFmNmNhYjIzNzM=|e4f699ffc63e2bb6068f8c6d544a7132eb4518bdf8c7344e850040146ef5d61f"', 'Pragma': 'no-cache', 'Cache-Control': 'no-cache', 'Sec-Websocket-Extensions': 'permessage-deflate; client_max_window_bits'})
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/web.py", line 1425, in _stack_context_handle_exception
        raise_exc_info((type, value, traceback))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py", line 314, in wrapped
        ret = fn(*args, **kwargs)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 191, in <lambda>
        self.on_recv(lambda msg: callback(self, msg), copy=copy)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py", line 373, in _on_zmq_reply
        super(ZMQChannelsHandler, self)._on_zmq_reply(stream, msg)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py", line 258, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/websocket.py", line 210, in write_message
        raise WebSocketClosedError()
    WebSocketClosedError
[I 05:51:39.838 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 05:52:39.670 NotebookApp] WebSocket ping timeout after 110611 ms.
[E 05:52:39.927 NotebookApp] Uncaught exception GET /api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E (218.241.251.148)
    HTTPServerRequest(protocol='http', host='ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', method='GET', uri='/api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E', version='HTTP/1.1', remote_ip='218.241.251.148', headers={'Origin': 'http://ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Upgrade': 'websocket', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch', 'Sec-Websocket-Version': '13', 'Host': 'ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Sec-Websocket-Key': 'GXOBxwMmwBjsa1pO4BuFCA==', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36', 'Connection': 'Upgrade', 'Cookie': '_xsrf=2|0144bb65|d2553a8ba249d96d8df5e6491fe6d9dd|1494811777; username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427="2|1:0|10:1494817375|67:username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427|44:OWVkYzZjNmY3MGU4NDI0YmFiMDQ2OGFmNmNhYjIzNzM=|e4f699ffc63e2bb6068f8c6d544a7132eb4518bdf8c7344e850040146ef5d61f"', 'Pragma': 'no-cache', 'Cache-Control': 'no-cache', 'Sec-Websocket-Extensions': 'permessage-deflate; client_max_window_bits'})
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/web.py", line 1425, in _stack_context_handle_exception
        raise_exc_info((type, value, traceback))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py", line 314, in wrapped
        ret = fn(*args, **kwargs)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 191, in <lambda>
        self.on_recv(lambda msg: callback(self, msg), copy=copy)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py", line 373, in _on_zmq_reply
        super(ZMQChannelsHandler, self)._on_zmq_reply(stream, msg)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py", line 258, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/websocket.py", line 210, in write_message
        raise WebSocketClosedError()
    WebSocketClosedError
[I 05:53:38.395 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:55:48.439 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 05:57:37.606 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 05:59:38.046 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:01:37.651 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:01:56.947 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:03:37.610 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:03:42.881 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:05:37.575 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:05:44.968 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:06:49.439 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:07:37.515 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:09:37.533 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:11:38.247 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:13:37.582 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:15:39.064 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:17:37.861 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:19:37.724 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:21:37.721 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:22:08.732 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:22:46.041 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:23:37.566 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:25:41.610 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:26:52.879 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:27:37.708 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[W 06:28:51.989 NotebookApp] Replacing stale connection: d77ced55-60db-4f58-be70-02fec45bfac1:80B51CEAAF294329951590B6EB139E1E
[I 06:29:11.153 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:29:35.262 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
[W 06:29:36.153 NotebookApp] Replacing stale connection: d77ced55-60db-4f58-be70-02fec45bfac1:80B51CEAAF294329951590B6EB139E1E
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[E 06:29:38.051 NotebookApp] Uncaught exception GET /api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E (218.241.251.148)
    HTTPServerRequest(protocol='http', host='ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', method='GET', uri='/api/kernels/d77ced55-60db-4f58-be70-02fec45bfac1/channels?session_id=80B51CEAAF294329951590B6EB139E1E', version='HTTP/1.1', remote_ip='218.241.251.148', headers={'Origin': 'http://ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Upgrade': 'websocket', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch', 'Sec-Websocket-Version': '13', 'Host': 'ec2-52-79-116-14.ap-northeast-2.compute.amazonaws.com:1427', 'Sec-Websocket-Key': 'bc5deV9nPYQkWHipRz+l0g==', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36', 'Connection': 'Upgrade', 'Cookie': '_xsrf=2|0144bb65|d2553a8ba249d96d8df5e6491fe6d9dd|1494811777; username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427="2|1:0|10:1494817375|67:username-ec2-52-79-116-14-ap-northeast-2-compute-amazonaws-com-1427|44:OWVkYzZjNmY3MGU4NDI0YmFiMDQ2OGFmNmNhYjIzNzM=|e4f699ffc63e2bb6068f8c6d544a7132eb4518bdf8c7344e850040146ef5d61f"', 'Pragma': 'no-cache', 'Cache-Control': 'no-cache', 'Sec-Websocket-Extensions': 'permessage-deflate; client_max_window_bits'})
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/web.py", line 1425, in _stack_context_handle_exception
        raise_exc_info((type, value, traceback))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py", line 314, in wrapped
        ret = fn(*args, **kwargs)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 191, in <lambda>
        self.on_recv(lambda msg: callback(self, msg), copy=copy)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py", line 373, in _on_zmq_reply
        super(ZMQChannelsHandler, self)._on_zmq_reply(stream, msg)
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py", line 258, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tornado/websocket.py", line 210, in write_message
        raise WebSocketClosedError()
    WebSocketClosedError
[W 06:29:38.239 NotebookApp] Replacing stale connection: d77ced55-60db-4f58-be70-02fec45bfac1:80B51CEAAF294329951590B6EB139E1E
[I 06:29:38.705 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:31:37.614 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:31:52.347 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:33:58.115 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:35:46.818 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:37:39.179 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:39:23.292 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:40:59.866 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:41:19.104 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 06:44:24.576 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:44:59.516 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:48:39.691 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 06:50:24.729 NotebookApp] Kernel restarted: d77ced55-60db-4f58-be70-02fec45bfac1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 06:51:03.279 NotebookApp] Kernel interrupted: d77ced55-60db-4f58-be70-02fec45bfac1
[I 06:52:19.196 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[C 07:17:10.521 NotebookApp] received signal 15, stopping
[I 07:17:10.521 NotebookApp] Shutting down kernels
[I 07:17:10.635 NotebookApp] Kernel shutdown: d77ced55-60db-4f58-be70-02fec45bfac1
[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[W 07:21:11.864 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 07:21:11.869 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 07:21:11.869 NotebookApp] 0 active kernels 
[I 07:21:11.869 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 07:21:11.869 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 07:21:23.563 NotebookApp] Kernel started: 1b47389d-84d1-460d-bb0f-fa1a9079295a
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:21:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[C 07:23:22.644 NotebookApp] received signal 15, stopping
[I 07:23:22.644 NotebookApp] Shutting down kernels
[I 07:23:23.654 NotebookApp] Kernel shutdown: 1b47389d-84d1-460d-bb0f-fa1a9079295a
[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future
[W 07:29:57.624 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 07:29:57.629 NotebookApp] Serving notebooks from local directory: /home/ubuntu/gtest/dist-keras
[I 07:29:57.629 NotebookApp] 0 active kernels 
[I 07:29:57.629 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:1427/
[I 07:29:57.630 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 07:30:05.674 NotebookApp] Kernel started: e19b59b9-7777-4cb8-851f-aaa6704c5b32
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:30:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 07:31:46.210 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
17/05/15 07:31:47 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: Could not read footer: java.lang.RuntimeException: file:/home/ubuntu/gtest/dist-keras/scripts/create and load secret file.ipynb is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [50, 10, 125, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:248)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$19.apply(ParquetFileFormat.scala:606)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$19.apply(ParquetFileFormat.scala:595)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: file:/home/ubuntu/gtest/dist-keras/scripts/create and load secret file.ipynb is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [50, 10, 125, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:238)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:234)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
17/05/15 07:31:47 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Could not read footer: java.lang.RuntimeException: file:/home/ubuntu/gtest/dist-keras/scripts/create and load secret file.ipynb is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [50, 10, 125, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:248)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$19.apply(ParquetFileFormat.scala:606)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$19.apply(ParquetFileFormat.scala:595)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: file:/home/ubuntu/gtest/dist-keras/scripts/create and load secret file.ipynb is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [50, 10, 125, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:238)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:234)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

17/05/15 07:31:47 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[I 07:32:06.355 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:32:09.630 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:34:05.903 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:41:12.965 NotebookApp] Kernel interrupted: e19b59b9-7777-4cb8-851f-aaa6704c5b32
[I 07:41:14.675 NotebookApp] KernelRestarter: restarting kernel (1/5)
WARNING:root:kernel e19b59b9-7777-4cb8-851f-aaa6704c5b32 restarted
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:41:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 07:41:19.736 NotebookApp] Kernel restarted: e19b59b9-7777-4cb8-851f-aaa6704c5b32
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:41:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 07:42:06.257 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:44:05.933 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:46:06.363 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:48:06.384 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:50:06.762 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:52:36.835 NotebookApp] Kernel restarted: e19b59b9-7777-4cb8-851f-aaa6704c5b32
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:52:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 07:53:10.974 NotebookApp] Kernel restarted: e19b59b9-7777-4cb8-851f-aaa6704c5b32
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:53:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 07:54:05.885 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:55:18.191 NotebookApp] Kernel restarted: e19b59b9-7777-4cb8-851f-aaa6704c5b32
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/15 07:55:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Using TensorFlow backend.
Exception in thread "main" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:256)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:110)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Traceback (most recent call last):
  File "/home/ubuntu/jobs/3Q20LA3MXU3N8Y9NVJ7A1T5WNHL2IWQSNNJ5V9I5P7MRJ8LSC33EN2DT3EWYLCJA.py", line 28, in <module>
    sc = SparkContext(conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 115, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/context.py", line 256, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/ubuntu/Download/spark-2.1.0/python/pyspark/java_gateway.py", line 95, in launch_gateway
    raise Exception("Java gateway process exited before sending the driver its port number")
Exception: Java gateway process exited before sending the driver its port number
[I 07:56:06.380 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 07:58:06.379 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
[I 08:00:06.434 NotebookApp] Saving file at /scripts/create and load secret file.ipynb
